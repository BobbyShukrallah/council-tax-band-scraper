import scrapyfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorfrom council_scrape.items import Homefrom scrapy import Requestimport reimport pandas as pdfrom requests import getimport jsonimport os#This is the main spider. You can run this in terminal by executing the command            # scrapy crawl england -a local_authority="Lewisham" -o  "Data/Lewisham.csv"            #This will get all council tax bands for properties in Lewisham, and then store them#in the folder Data as a csv file called "Lewisham.csv"            # Note that spider arguments can be passed in the crawl command using the -a option. In the #above example, the local authority "Lewisham" is now stored in self.local_authority            class England(scrapy.Spider):                #Name of spider, spiders are run in terminal by calling "scrapy crawl NAME"    name= "england"        #We set the local authority name as the index of the dataframe, since we will    #look up one of the "lstBa" codes from befire by just callingm say "codes[Lewisham]"                #This is the main webpage with the form to be filled out    start_urls = ['http://cti.voa.gov.uk/cti/RefS.asp?lcn=0']        """        The webpage for finding information on council tax bands is http://cti.voa.gov.uk/.    There is a form there where, traditionally, one would fill it out by hand and submit a search.    Instead of filling it out by hand, we look at the form data being sent via    POST. We can see what these values are by looking at the network activity when    submitting the form by hand (on chrome, right click the page and select "inspect",    click "network", then submit the form for some different parameters. You'll see    some action in the network panel, click on the "RefS.asp" tab and scroll down,    you'll see how the data from your form is being read and posted (as a dictionary).     We copy that data and send it in a form request using scrapy. Some of that data is    independent of our request, so we make a dictionary of it below to incorporate    into any specific request.)    """        data = {                #  ,                "btnPush":"1",                "txtRedirectTo":"RefS.asp",                "txtPageSize": "50",        #Number of results per page, we want the max for easy scraping                "txtPageNum":"1",           #Page number, we'll change this as we go so we can iterate through each page of results                "lstPartDomestic": "N"      #This says we only want domestic properties                }                def parse(self, response):                                self.LA_codes = pd.DataFrame(columns = ["Local Authority","lstBa"])                        """        When filling out the form on the webpage, the data on your form is sent to retrieve results.        In the form submitted to the website, there is a key "lstBA" with a number        that corrsponds to the id of the local authority in the dropdown list of the menu.         We first obtain the list of local authorities and their codes and store these        in self.LA_codes                """        for LA in response.xpath('//select[@id = "lstBA"]/option'):            name = LA.xpath('text()').get()            if name:                name=name.strip()                row = pd.DataFrame({"Local Authority":[name],                   "lstBa":[LA.xpath('@value').get()]})                self.LA_codes  = pd.concat([self.LA_codes ,row],ignore_index=True)                        self.LA_codes.set_index("Local Authority",inplace=True)                                """        The data dictionary above is the form data we will send in our reqest to         simulate filling out a form. We now start adding our search specific criteria.        Here, we add the name of our local authority as well as the dropdown menu code.        """        self.data['txtBAName']= self.local_authority.upper()                      self.data['lstBA'] =  self.LA_codes.loc[self.local_authority.upper()]        """        The website also requires that your search is specific enough, it isn't possible        to just apply a search for one local authority. It is too many requests        to simply go through every postcode in a local authority, but what you can         do is pass a partial postcode, like "AB1 2" instead of "AB1 2CD", and this will        return all addresses which have a postcode starting with "AB1 2". However,        these can spill into other local authorities, which is why we submit in our         form data both the local authority and a partial postcode.         We import a list of shortened postcodes and their corresponding local authorities.         """        LA_postcodes_df = pd.read_csv("Data/local_authorities_postcodes.csv")        #We restrict the dataframe to just the postcodes in our local authority.         LA_postcodes_df = LA_postcodes_df[LA_postcodes_df["Name"].str.lower() == self.local_authority.lower()]                #Finally, we just select the postcodes column.        self.postcodes = LA_postcodes_df["short_postcodes"]                """        Now we make a dictionary of dictionaries, where each key is a shortened        postcode and the item is a copy of the data dictionary above. Thus, the         formdata for a search of a specific postcode that will be submit is the         dictionary formdata[postcode]. Of course these are all the same, but below        we will change the "['txtPostCode']" entry to be the postcode.                 The reason for this roundabout way is that we need to pass the formdata        as metadata for each request, including the postcode, but if we instead edited         the same data dictionary each time we yielded a request and change the 'txtPostCode'        entry each time, then this could change the postcode data for other yielded        requests.         """                formdata = {postcode:self.data.copy() for postcode in self.postcodes}                                        for postcode in self.postcodes:                        formdata[postcode]['txtPostCode']  = postcode                        #Just printing some text to let me know what's going on.            print("Yielding " + postcode)            """            This tells scrapy to send a form request using the formdata for the            given postcode and then process the page with "self.process_page" below            """            yield scrapy.FormRequest.from_response(                response,                formdata=formdata[postcode],                callback=self.process_page,                meta={"formdata":formdata,                      "postcode":postcode}        )        #This is the method that actually scrapes the data off of a given page                def process_page(self,response):                postcode = response.meta["postcode"]                formdata = response.meta["formdata"]                form = formdata[postcode]                        #This keeps track of what page we're on. For the first request for this        #postcode, this will just be the number 1.                page = int(form["txtPageNum"])        #Each address is listed in a row on the page. We collect the data and         #yield it. It will then be written as a row in a csv file.                rows = response.xpath('//tr')        for row in rows:            row_items = row.xpath('td')            if row_items:                yield {"address": row_items[0].xpath('span/a/text()').re(r'[^\n^\s].+'),                        "band":row_items[1].xpath('text()').re(r'([a-zA-Z]+)'),                       "page":str(page),                       "code":form['txtPostCode']                      }                                        number_of_results = int(response.xpath('//p//strong/text()').re(r"Showing.+of ([0-9]+)")[0] )                pages = (number_of_results // 50 ) +1                #Print the postcode and which page of total pages we're on.                print(f"{form['txtPostCode']} page {page} of {pages}")        #If we are not at the last page, we submit the form again only now we        #increase the page number by 1 to go to the next page.                 if pages>page:            page+=1            form["txtPageNum"]=str(page)                                    yield scrapy.FormRequest(url="http://cti.voa.gov.uk/cti/RefS.asp?lcn=0",                formdata=form,                callback=self.process_page,                meta = {"formdata":formdata, "postcode":postcode})                                                                    